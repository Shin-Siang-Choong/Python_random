{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "wpt = nltk.tokenize.TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_counter_sum(counter_list):\n",
    "    counter_final = Counter()\n",
    "    [counter_final.update(x) for x in counter_list]\n",
    "    return counter_final\n",
    "\n",
    "def ngram_counter(doc_list, ngram, tokenizer):\n",
    "    vectorizer = CountVectorizer(ngram_range=(ngram,ngram), tokenizer=tokenizer)\n",
    "    analyzer = vectorizer.build_analyzer()\n",
    "    tokenized_list = [analyzer(x) for x in doc_list]\n",
    "    test = [Counter(z) for z in tokenized_list]\n",
    "    return {ngram: sum(test, Counter())}\n",
    "\n",
    "def ngram_counter_summary(doc_list, min_ngram, max_ngram, tokenizer, num_process=1, parallel=False):\n",
    "    \n",
    "    if parallel:\n",
    "        assert num_process is not None, \"Please specify number of processes!\"\n",
    "        main_doc_list = np.array_split(doc_list, num_process)\n",
    "        ngram_dict_list = []\n",
    "        for i in range(min_ngram,max_ngram + 1):\n",
    "            ngram_counter_dict_list = Parallel(n_jobs=num_process)(delayed(ngram_counter)(x,i, tokenizer) for x in main_doc_list)\n",
    "            ngram_dict_list = ngram_dict_list + ngram_counter_dict_list\n",
    "            \n",
    "        a={}\n",
    "        for d in ngram_dict_list:\n",
    "            for k,v in d.items():\n",
    "                try:\n",
    "                    a[k].append(v)\n",
    "                except KeyError:\n",
    "                    a[k]=[v]\n",
    "                    \n",
    "        ngram_counter_dict = dict((k,list_counter_sum(v)) for k,v in a.items())\n",
    "#         ngram_counter_dict = a\n",
    "        \n",
    "    else:\n",
    "        ngram_counter_dict = {}\n",
    "        for i in range(min_ngram, max_ngram + 1):\n",
    "            ngram_counter_dict.update(ngram_counter(doc_list, i, tokenizer))\n",
    "    \n",
    "    return ngram_counter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list = [\n",
    "    'This is an example',\n",
    "    'This is just an example',\n",
    "    'This is just a fucking example',\n",
    "    'This is just a fucking noob example',\n",
    "    'This is just a damn fucking noob example'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: Counter({'this': 5,\n",
       "          'is': 5,\n",
       "          'an': 2,\n",
       "          'example': 5,\n",
       "          'just': 4,\n",
       "          'a': 3,\n",
       "          'fucking': 3,\n",
       "          'noob': 2,\n",
       "          'damn': 1}),\n",
       " 2: Counter({'this is': 5,\n",
       "          'is an': 1,\n",
       "          'an example': 2,\n",
       "          'is just': 4,\n",
       "          'just an': 1,\n",
       "          'just a': 3,\n",
       "          'a fucking': 3,\n",
       "          'fucking example': 1,\n",
       "          'fucking noob': 1,\n",
       "          'noob example': 2,\n",
       "          'fucking damn': 1,\n",
       "          'damn noob': 1}),\n",
       " 3: Counter({'this is an': 1,\n",
       "          'is an example': 1,\n",
       "          'this is just': 4,\n",
       "          'is just an': 1,\n",
       "          'just an example': 1,\n",
       "          'is just a': 3,\n",
       "          'just a fucking': 3,\n",
       "          'a fucking example': 1,\n",
       "          'a fucking noob': 1,\n",
       "          'fucking noob example': 1,\n",
       "          'a fucking damn': 1,\n",
       "          'fucking damn noob': 1,\n",
       "          'damn noob example': 1})}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_counter_summary(string_list,1,3,wpt.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this is', 5), ('is just', 4), ('just a', 3)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = 3\n",
    "ngram = 2\n",
    "ngram_counter(string_list,ngram,wpt.tokenize)[ngram].most_common()[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
